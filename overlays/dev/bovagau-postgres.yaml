apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: bovagau-postgres
  namespace: bovagau
#  labels:
#    application: test-app
#    environment: demo
#  annotations:
#    "acid.zalan.do/controller": "second-operator"
#    "delete-date": "2020-08-31"  # can only be deleted on that day if "delete-date "key is configured
#    "delete-clustername": "acid-test-cluster"  # can only be deleted when name matches if "delete-clustername" key is configured
spec:
  dockerImage: registry.opensource.zalan.do/acid/spilo-14:2.1-p6
  teamId: bovagau
  numberOfInstances: 3
  clone:
    uid: "a06e205d-6820-49e6-a089-9d88fd424a79"
    cluster: "bovagau-postgres"
    timestamp: "2022-12-22T19:22:00+07:00"
  users: # Application/Robot users
    bovagau: []
  #    foo_user: []
  #    flyway: []
  #  usersWithSecretRotation:
  #  - foo_user
  #  usersWithInPlaceSecretRotation:
  #  - flyway
  #  - bar_owner_user
  enableMasterLoadBalancer: false
  enableReplicaLoadBalancer: false
  enableConnectionPooler: true # enable/disable connection pooler deployment
  enableReplicaConnectionPooler: true # set to enable connectionPooler for replica service
  allowedSourceRanges:  # load balancers' source ranges for both master and replica services
    - 127.0.0.1/32
  databases:
    bovagau: bovagau
  postgresql:
    version: "14"
    parameters:  # Expert section
      log_statement: "ddl"
      max_connections: "1000"
      shared_buffers: 4GB
      effective_cache_size: 12GB
      maintenance_work_mem: 1GB
      checkpoint_completion_target: "0.9"
      wal_buffers: 16MB
      default_statistics_target: "100"
      random_page_cost: "1.1"
      effective_io_concurrency: "300"
      work_mem: 2097kB
      min_wal_size: 1GB
      max_wal_size: 4GB
      max_worker_processes: "4"
      max_parallel_workers_per_gather: "2"
      max_parallel_workers: "4"
      max_parallel_maintenance_workers: "2"
  #  env:
  #  - name: wal_s3_bucket
  #    value: my-custom-bucket

  volume:
    size: 24Gi
    storageClass: ceph-rbd-nvme
  #    storageClass: my-sc
  #    iops: 1000  # for EBS gp3
  #    throughput: 250  # in MB/s for EBS gp3
  #    selector:
  #      matchExpressions:
  #        - { key: flavour, operator: In, values: [ "banana", "chocolate" ] }
  #      matchLabels:
  #        environment: dev
  #        service: postgres
  #  additionalVolumes:
  #    - name: empty
  #      mountPath: /opt/empty
  #      targetContainers:
  #        - all
  #      volumeSource:
  #        emptyDir: {}
  #    - name: data
  #      mountPath: /home/postgres/pgdata/partitions
  #      targetContainers:
  #        - postgres
  #      volumeSource:
  #        PersistentVolumeClaim:
  #          claimName: pvc-postgresql-data-partitions
  #          readyOnly: false
  #    - name: conf
  #      mountPath: /etc/telegraf
  #      subPath: telegraf.conf
  #      targetContainers:
  #        - telegraf-sidecar
  #      volumeSource:
  #        configMap:
  #          name: my-config-map

  enableShmVolume: true
  #  spiloRunAsUser: 101
  #  spiloRunAsGroup: 103
  #  spiloFSGroup: 103
  #  podAnnotations:
  #    annotation.key: value
  #  serviceAnnotations:
  #    annotation.key: value
  #  podPriorityClassName: "spilo-pod-priority"
  #  tolerations:
  #  - key: postgres
  #    operator: Exists
  #    effect: NoSchedule
  resources:
    limits:
      cpu: "4"
      memory: 16Gi
    requests:
      cpu: 500m
      memory: 2Gi
  patroni:
    initdb:
      encoding: "UTF8"
      locale: "en_US.UTF-8"
      data-checksums: "true"
    pg_hba:
      - local       all             all                             trust
      - hostssl     all             +zalandos    127.0.0.1/32       pam
      - host        all             all          127.0.0.1/32       md5
      - hostssl     all             +zalandos    ::1/128            pam
      - host        all             all          ::1/128            md5
      - local       replication     standby                         trust
      - hostssl     replication     standby      all                md5
      - hostssl     all             +zalandos    all                pam
      - hostssl     all             all          10.0.0.0/8         md5
      - host        all             all          10.0.0.0/8         md5
      - hostnossl   all             all          all                reject
    #    slots:
    #      permanent_physical_1:
    #        type: physical
    #      permanent_logical_1:
    #        type: logical
    #        database: foo
    #        plugin: pgoutput
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    synchronous_mode: false
    synchronous_mode_strict: false
    synchronous_node_count: 1
    maximum_lag_on_failover: 33554432

  # restore a Postgres DB with point-in-time-recovery
  # with a non-empty timestamp, clone from an S3 bucket using the latest backup before the timestamp
  # with an empty/absent timestamp, clone from an existing alive cluster using pg_basebackup
  #  clone:
  #    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
  #    cluster: "acid-minimal-cluster"
  #    timestamp: "2017-12-19T12:40:33+01:00"  # timezone required (offset relative to UTC, see RFC 3339 section 5.6)
  #    s3_wal_path: "s3://custom/path/to/bucket"

  # run periodic backups with k8s cron jobs
  enableLogicalBackup: true
  #  logicalBackupSchedule: "30 00 * * *"

  #  maintenanceWindows:
  #  - 01:00-06:00  #UTC
  #  - Sat:00:00-04:00
  #  podAnnotations:
  #    linkerd.io/inject: enabled

  # overwrite custom properties for connection pooler deployments
  #  connectionPooler:
  #    numberOfInstances: 2
  #    mode: "transaction"
  #    schema: "pooler"
  #    user: "pooler"
  #    resources:
  #      requests:
  #        cpu: 300m
  #        memory: 100Mi
  #      limits:
  #        cpu: "1"
  #        memory: 100Mi

  #  initContainers:
  #    - name: date
  #      image: busybox
  #      command: [ "/bin/date" ]
  #  sidecars:
  #    - name: pmm-client
  #      image: 'percona/pmm-client:2.30.0'
  #      ports:
  #        - containerPort: 7777
  #          protocol: TCP
  #      env:
  #        - name: PMM_SERVER
  #          value: pmm-monitoring-service.operators-system
  #        - name: DB_TYPE
  #          value: postgresql
  #        - name: DB_USER
  #          valueFrom:
  #            secretKeyRef:
  #              name: postgres.bovagau-postgres.credentials
  #              key: username
  #        - name: DB_PASSWORD
  #          valueFrom:
  #            secretKeyRef:
  #              name: postgres.bovagau-postgres.credentials
  #              key: password
  #        - name: DB_HOST
  #          value: localhost
  #        - name: DB_CLUSTER
  #          value: bovagau-postgres
  #        - name: DB_PORT
  #          value: '5432'
  #        - name: PMM_USER
  #          value: api_key
  #        - name: PMM_PASSWORD
  #          value: "eyJrIjoiYkVyZ0NXQ1h1TERMczBkUVNoaTN6NzRKNGdrQWRMaHEiLCJuIjoicG1tLWNsaWVudC1ib3ZhZ2F1LXBvc3RncmVzIiwiaWQiOjF9"
  #        - name: POD_NAME
  #          valueFrom:
  #            fieldRef:
  #              apiVersion: v1
  #              fieldPath: metadata.name
  #        - name: POD_NAMESPASE
  #          valueFrom:
  #            fieldRef:
  #              apiVersion: v1
  #              fieldPath: metadata.namespace
  #        - name: PMM_AGENT_SERVER_ADDRESS
  #          value: pmm-monitoring-service.operators-system
  #        - name: PMM_AGENT_LISTEN_PORT
  #          value: '7777'
  #        - name: PMM_AGENT_CONFIG_FILE
  #          value: /usr/local/percona/pmm2/config/pmm-agent.yaml
  #        - name: PMM_AGENT_SERVER_INSECURE_TLS
  #          value: '1'
  #        - name: PMM_AGENT_LISTEN_ADDRESS
  #          value: 0.0.0.0
  #        - name: PMM_AGENT_SETUP_NODE_NAME
  #          value: $(POD_NAMESPASE)-$(POD_NAME)
  #        - name: PMM_AGENT_SETUP
  #          value: '1'
  #        - name: PMM_AGENT_SETUP_FORCE
  #          value: '1'
  #        - name: PMM_AGENT_SETUP_NODE_TYPE
  #          value: container
  #        - name: PMM_AGENT_SETUP_METRICS_MODE
  #          value: push
  #        - name: PMM_ADMIN_CUSTOM_PARAMS
  #        - name: PMM_AGENT_SERVER_USERNAME
  #          value: api_key
  #        - name: PMM_AGENT_SERVER_PASSWORD
  #          value: "eyJrIjoiYkVyZ0NXQ1h1TERMczBkUVNoaTN6NzRKNGdrQWRMaHEiLCJuIjoicG1tLWNsaWVudC1ib3ZhZ2F1LXBvc3RncmVzIiwiaWQiOjF9"
  #        - name: CLUSTER_NAME
  #          value: bovagau-postgres
  #        - name: PMM_AGENT_PRERUN_SCRIPT
  #          value: >-
  #            pmm-admin status --wait=10s;
  #
  #            pmm-admin add $(DB_TYPE) $(PMM_ADMIN_CUSTOM_PARAMS)
  #            --skip-connection-check --metrics-mode=push  --username=$(DB_USER)
  #            --password=$(DB_PASSWORD) --cluster=$(CLUSTER_NAME)
  #            --service-name=$(PMM_AGENT_SETUP_NODE_NAME) --host=$(DB_HOST)
  #            --port=$(DB_PORT);
  #
  #            pmm-admin annotate --service-name=$(PMM_AGENT_SETUP_NODE_NAME)
  #            'Service restarted'
  #        - name: PMM_AGENT_SIDECAR
  #          value: 'true'
  #        - name: PMM_AGENT_SIDECAR_SLEEP
  #          value: '5'
  #      resources:
  #        limits:
  #          cpu: '1'
  #          memory: 2Gi
  #        requests:
  #          cpu: '50m'
  #          memory: 50Mi
  #      volumeMounts:
  #        - name: kube-api-access-vl7w7
  #          readOnly: true
  #          mountPath: /var/run/secrets/kubernetes.io/serviceaccount
  #      livenessProbe:
  #        httpGet:
  #          path: /local/Status
  #          port: 7777
  #          scheme: HTTP
  #        initialDelaySeconds: 60
  #        timeoutSeconds: 5
  #        periodSeconds: 10
  #        successThreshold: 1
  #        failureThreshold: 3
  #      lifecycle:
  #        preStop:
  #          exec:
  #            command:
  #              - bash
  #              - '-c'
  #              - >-
  #                pmm-admin inventory remove node --force $(pmm-admin status
  #                --json | python -c "import sys, json;
  #                print(json.load(sys.stdin)['pmm_agent_status']['node_id'])")
  #      terminationMessagePath: /dev/termination-log
  #      terminationMessagePolicy: File
  #      imagePullPolicy: Always
  #
  # Custom TLS certificate. Disabled unless tls.secretName has a value.
#  tls:
#    secretName: ""  # should correspond to a Kubernetes Secret resource to load
#    certificateFile: "tls.crt"
#    privateKeyFile: "tls.key"
#    caFile: ""  # optionally configure Postgres with a CA certificate
#    caSecretName: "" # optionally the ca.crt can come from this secret instead.
# file names can be also defined with absolute path, and will no longer be relative
# to the "/tls/" path where the secret is being mounted by default, and "/tlsca/"
# where the caSecret is mounted by default.
# When TLS is enabled, also set spiloFSGroup parameter above to the relevant value.
# if unknown, set it to 103 which is the usual value in the default spilo images.
# In Openshift, there is no need to set spiloFSGroup/spilo_fsgroup.

# Add node affinity support by allowing postgres pods to schedule only on nodes that
# have label: "postgres-operator:enabled" set.
#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#        - matchExpressions:
#            - key: postgres-operator
#              operator: In
#              values:
#                - enabled

# Enables change data capture streams for defined database tables
#  streams:
#  - applicationId: test-app
#    database: foo
#    tables:
#      data.state_pending_outbox:
#        eventType: test-app.status-pending
#      data.state_approved_outbox:
#        eventType: test-app.status-approved
#      data.orders_outbox:
#        eventType: test-app.order-completed
#        idColumn: o_id
#        payloadColumn: o_payload
#    # Optional. Filter ignores events before a certain txnId and lsn. Can be used to skip bad events
#    filter:
#      data.orders_outbox: "[?(@.source.txId > 500 && @.source.lsn > 123456)]"
#    batchSize: 1000
